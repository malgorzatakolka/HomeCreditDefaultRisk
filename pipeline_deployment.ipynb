{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fa2ec2-c980-412b-95b9-08af56f552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pandas.core.frame import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder, RareLabelEncoder\n",
    "from feature_engine.creation import MathFeatures\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "from feature_engine.selection import (\n",
    "    SmartCorrelatedSelection,\n",
    "    DropHighPSIFeatures,\n",
    "    SelectBySingleFeaturePerformance,\n",
    ")\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cdecb-d816-442e-9018-325710370026",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = load_data(\"application_train\")\n",
    "merged = pd.read_csv('preprocessed_data/merged.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "735b9a3c-0521-48e3-93cf-db24420a7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organization_replacer(value: any) -> any:\n",
    "    \"\"\"Reduces the number of unique values\n",
    "    where there are subcategories with ':' sign\"\"\"\n",
    "\n",
    "    if value not in [np.nan, None]:\n",
    "        x = value.split()[0]\n",
    "        if x[-1] == \":\":\n",
    "            return x[:-1]\n",
    "        elif x == \"Business\":\n",
    "            return \"Business\"\n",
    "    return value\n",
    "\n",
    "def remove_special_chars(s: str) -> str:\n",
    "    \"\"\"Replaces special characters from string with '_'.\"\"\"\n",
    "\n",
    "    return \"\".join(e if e.isalnum() else \"_\" for e in s)\n",
    "\n",
    "\n",
    "def change_sign(value: [int, float]) -> [int, float]:\n",
    "    \"\"\"Changes sign of negative numerical values.\"\"\"\n",
    "\n",
    "    if value < 0:\n",
    "        return value * (-1)\n",
    "\n",
    "\n",
    "def y_n_encode(value: str) -> int:\n",
    "    \"\"\"Encodes values 'Y' with 1 and 'N' with 0.\"\"\"\n",
    "\n",
    "    if value == \"Y\":\n",
    "        return 1\n",
    "    elif value == \"N\":\n",
    "        return 0\n",
    "\n",
    "def nn_mean(x: DataFrame, X_tain_prep: DataFrame, y_train: pd.Series) -> DataFrame:\n",
    "    \"\"\"Adds two columns to DataFrame of mean values of target for 50 and 100\n",
    "    nearest neighbors od the poin from training set.\n",
    "    Parameters: x (DataFrame to be transformer),\n",
    "                X_train_prep (preprocessed DataFrame to be fitted to NearestNeighbors model)\n",
    "                y_train (Series of target values to be used to calculate means).\"\"\"\n",
    "    \n",
    "    # Getting columns of interest\n",
    "    columns_of_int = [\n",
    "        \"EXT_SOURCE_1\",\n",
    "        \"EXT_SOURCE_2\",\n",
    "        \"EXT_SOURCE_3\",\n",
    "        \"AMT_CREDIT\",\n",
    "        \"AMT_ANNUITY\",\n",
    "    ]\n",
    "    # Getting data for fitting\n",
    "    df_nn = X_train_prep[columns_of_int]\n",
    "    df_nn[\"CREDIT_ANNUITY_RATIO\"] =  df_nn[\"AMT_CREDIT\"] / (df_nn[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # Getting data for neighbors\n",
    "    df_get = x[columns_of_int]\n",
    "    df_get[\"CREDIT_ANNUITY_RATIO\"] =  df_get[\"AMT_CREDIT\"] / (df_get[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # 50 neighbors\n",
    "    # Fitting model with 50 neighbors\n",
    "    nn_50 = NearestNeighbors(n_neighbors=50).fit(df_nn)\n",
    "    # Indices of neighbours\n",
    "    train_50_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    # Calculating means\n",
    "    new_column_1 = [y_train.iloc[ind].mean() for ind in train_50_neighbours]\n",
    "    # Adding column\n",
    "    x[\"MEAN_50_NN\"] = new_column_1\n",
    "\n",
    "    # 100 neighbors\n",
    "    nn_100 = NearestNeighbors(n_neighbors=100).fit(df_nn)\n",
    "    train_100_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    new_column_2 = [y_train.iloc[ind].mean() for ind in train_100_neighbours]\n",
    "    x[\"MEAN_100_NN\"] = new_column_2\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def devision(x: List) -> int:\n",
    "    \"\"\"Devides two features from the list\n",
    "    avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return x[0] / (x[1] + 0.001)\n",
    "\n",
    "\n",
    "def sum_dev(x: List) -> int:\n",
    "    \"\"\"Performs three features math operation\n",
    "    from the list avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return (x[0] + x[1]) * x[2] / 2\n",
    "\n",
    "\n",
    "def weighted_mul(x: List) -> int:\n",
    "    \"\"\"Gets weighted sum of three values in a list.\"\"\"\n",
    "    return x[0] * 2 + x[1] * 3 + x[2] * 4\n",
    "    \n",
    "# Assign ordinality to NAME_EDUCATION_TYPE\n",
    "def encode_education(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assigns ordinality to NAME_EDUCATION_TYPE column\"\"\"\n",
    "    \n",
    "    education = {\n",
    "        \"Secondary / secondary special\": 1,\n",
    "        \"Higher education\": 3,\n",
    "        \"Incomplete higher\": 2,\n",
    "        \"Lower secondary\": 0,\n",
    "        \"Academic degree\": 4,\n",
    "    }\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = df[\"NAME_EDUCATION_TYPE\"].map(education)\n",
    "    \n",
    "    return  df\n",
    "\n",
    "def merging(df):\n",
    "    df = df.merge(merged, on='SK_ID_CURR', how='left')\n",
    "    df = df.fillna(0)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def getting_model_columns(df):\n",
    "\n",
    "    # Read column names from the text file\n",
    "    with open('column_names.txt', 'r') as file:\n",
    "        column_names = file.read().splitlines()\n",
    "    return df[column_names]\n",
    "\n",
    "\n",
    "def organization(df):\n",
    "    df[\"ORGANIZATION_TYPE\"] = df[\"ORGANIZATION_TYPE\"].map(\n",
    "    organization_replacer)\n",
    "    return df\n",
    "\n",
    "\n",
    "def gender_replacer(df):\n",
    "    df[\"CODE_GENDER\"].replace({\"XNA\": np.nan, \"M\": 0, \"F\": 1}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sign_change(df):\n",
    "    for col in [\"DAYS_BIRTH\", \"DAYS_LAST_PHONE_CHANGE\", \"DAYS_ID_PUBLISH\", \"DAYS_EMPLOYED\"]:\n",
    "        df[col] = df[col].map(change_sign)\n",
    "    return df\n",
    "\n",
    "# ======================= Getting data =======================\n",
    "def reduce_memory_usage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Reduced memory usage by downcasting datatype of columns.\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\"\"\"\n",
    "\n",
    "    # Downcasting dataframe\n",
    "    for column in df:\n",
    "        if df[column].dtype in [\"float64\", \"float32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"float\")\n",
    "        if df[column].dtype in [\"int64\", \"int32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(name: str) -> DataFrame:\n",
    "    \"\"\"Loads DataFrame from csv and reduces used memory.\n",
    "    Parameters: name (the name of csv file without .csv extension)\n",
    "    Returns: DataFrame\"\"\"\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{name}.csv loading\")\n",
    "    df = pd.read_csv(f\"{name}.csv\")\n",
    "    memory = df.memory_usage().sum() / 1024**2\n",
    "    df = reduce_memory_usage(df)\n",
    "    print(\n",
    "        f\"memory usage reduced from {memory:.1f}MB to {(df.memory_usage().sum() / 1024**2):.1f}MB\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1beba70a-fd4e-46a8-87ca-b857d3420ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_app = app.iloc[:1000, :]\n",
    "small_app.head(2)\n",
    "X_train = small_app\n",
    "y_train = X_train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "15e55d7b-23d6-4851-a853-eb051ed42754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "744587e2-54ac-4939-9b04-296710798ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pipe = Pipeline(steps=[\n",
    "                                ('education_type', FunctionTransformer(encode_education)),\n",
    "                                ('organization', FunctionTransformer(organization)), \n",
    "                                ('gender', FunctionTransformer(gender_replacer)),\n",
    "                                ('sign_change', FunctionTransformer(sign_change)),\n",
    "                                ('ewd', EqualWidthDiscretiser(bins=6, variables=[\"HOUR_APPR_PROCESS_START\"])),\n",
    "])\n",
    "\n",
    "preprocess_pipe = Pipeline(steps=[\n",
    "                                # Reduce cardinality\n",
    "                                (\"rle\", RareLabelEncoder(tol=0.04, n_categories=1, variables=categorical, missing_values=\"ignore\")),\n",
    "                                # Replace NA by the median in numerical features\n",
    "                                (\n",
    "                                    \"continuous_var_median_imputer\",\n",
    "                                    MeanMedianImputer(imputation_method=\"median\", variables=numerical),\n",
    "                                ),\n",
    "                                # Replace NA in discrete variables by most frequent value\n",
    "                                (\n",
    "                                    \"discreate_var_mode_imputer\",\n",
    "                                    CategoricalImputer(\n",
    "                                        imputation_method=\"frequent\", ignore_format=True, variables=discrete\n",
    "                                    ),\n",
    "                                ),\n",
    "                                # Replace NA by adding the label \"Missing\" in categorical variables\n",
    "                                (\n",
    "                                    \"categorical_imputer\",\n",
    "                                    CategoricalImputer(fill_value=\"XNA\", variables=categorical),\n",
    "                                ),\n",
    "                                # Encode categorical variables using one hot encoding\n",
    "                                (\"one_hot_encoder\", OneHotEncoder(variables=categorical)),\n",
    "])\n",
    "\n",
    "feature_pipe = Pipeline(\n",
    "    steps=[\n",
    "        # Adding features\n",
    "        (\n",
    "            \"f_1\",\n",
    "            MathFeatures(\n",
    "                [\"DAYS_EMPLOYED\", \"DAYS_BIRTH\"], devision, [\"RATIO_EMPLOYED_TO_AGE\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_2\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_ANNUITY\", \"AMT_INCOME_TOTAL\"],\n",
    "                devision,\n",
    "                [\"RATIO_ANNUITY_TO_INCOME\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_3\",\n",
    "            MathFeatures(\n",
    "                [\n",
    "                    \"REGION_RATING_CLIENT\",\n",
    "                    \"REGION_RATING_CLIENT_W_CITY\",\n",
    "                    \"AMT_INCOME_TOTAL\",\n",
    "                ],\n",
    "                sum_dev,\n",
    "                [\"REGION_TO_INCOME\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_4\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_INCOME_TOTAL\", \"EXT_SOURCE_3\"],\n",
    "                devision,\n",
    "                [\"RATIO_INCOME_TO_EXT_SOURCE\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_5\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_CREDIT\", \"EXT_SOURCE_3\"], devision, [\"RATIO_CREDIT_TO_EXT_SOURCE\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_6\",\n",
    "            MathFeatures(\n",
    "                [\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\"],\n",
    "                [np.sum, np.mean, np.max, np.min, np.prod, weighted_mul],\n",
    "                [\n",
    "                    \"SUM_EXT_SOURCES\",\n",
    "                    \"MEAN_EXT_SOURCES\",\n",
    "                    \"MAX_EXTERNAL_SOURCES\",\n",
    "                    \"MIN_EXT_SOURCES\",\n",
    "                    \"PROD_EXT_SOURCES\",\n",
    "                    \"WEIGHTED_EXT_SOURCES\",\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_7\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_CREDIT\", \"AMT_ANNUITY\"], devision, [\"CREDIT_ANNUITY_RATIO\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_8\",\n",
    "            MathFeatures(\n",
    "                [\"REGION_POPULATION_RELATIVE\", \"AMT_CREDIT\"],\n",
    "                np.prod,\n",
    "                [\"PROD_REGION_POPULATION_AMT_CREDIT\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_9\",\n",
    "            MathFeatures(\n",
    "                [\"REGION_RATING_CLIENT\", \"AMT_INCOME_TOTAL\"],\n",
    "                np.prod,\n",
    "                [\"PROD_REGION_RATING_AMT_INCOME\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_10\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_INCOME_TOTAL\", \"CNT_CHILDREN\"], devision, [\"INCOME_PER_CHILD\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_11\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_INCOME_TOTAL\", \"CNT_FAM_MEMBERS\"], devision, [\"INCOME_PER_PERSON\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_12\",\n",
    "            MathFeatures(\n",
    "                [\"AMT_ANNUITY\", \"AMT_INCOME_TOTAL\"], devision, [\"ANNUITY_INCOME_PERC\"]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"f_13\",\n",
    "            MathFeatures([\"AMT_ANNUITY\", \"AMT_CREDIT\"], devision, [\"PAYMENT_RATE\"]),\n",
    "        ),\n",
    "        (\n",
    "            \"col_names\",\n",
    "            FunctionTransformer(lambda x: x.rename(columns=remove_special_chars)),\n",
    "        ),\n",
    "        (\"nearest_neighbours\", FunctionTransformer(nn_mean, kw_args={'X_tain_prep': X_train,\n",
    "                                                                     'y_train': y_train})\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "merging_pipe = Pipeline(steps=[\n",
    "                            # Merging with data from other sources\n",
    "                            ('merging', FunctionTransformer(merging)),\n",
    "                            # Getting columns that are used for model training\n",
    "                            ('getting_model_columns', FunctionTransformer(getting_model_columns))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a087266a-d113-4c6a-ad76-1f8378beff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_app = app.iloc[:1000, :]\n",
    "small_app.head(2)\n",
    "X_train = small_app\n",
    "y_train = X_train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0051-9548-41bc-8052-840984c647fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_columns = initial_pipe.fit_transform(X_train)\n",
    "\n",
    "# List of categorical variables\n",
    "categorical = [var for var in X_for_columns.columns if X_for_columns[var].dtype == \"O\"]\n",
    "\n",
    "# List of numerical variables\n",
    "numerical = [var for var in X_for_columns.columns if X_for_columns[var].dtype != \"O\"]\n",
    "\n",
    "# List of discrete variables\n",
    "discrete = [var for var in numerical if len(X_for_columns[var].unique()) < 20]\n",
    "\n",
    "# Continuous variables\n",
    "numerical = [var for var in numerical if var not in discrete]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a2951f5-6e62-4f84-a807-5433d143bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('initial_pipe', initial_pipe),\n",
    "    ('preprocess_pipe', preprocess_pipe),\n",
    "    ('feature_pipe', feature_pipe),\n",
    "    ('merging_pipe', merging_pipe)\n",
    "])\n",
    "\n",
    "X_train = pipe.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33dfa9a3-5481-40ed-bb5b-c555b4bee596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e9fed0d8-eb85-4555-bd77-2f11e03e373a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.702756  , 0.297244  ],\n",
       "       [0.96980566, 0.03019435],\n",
       "       [0.9596382 , 0.04036184],\n",
       "       ...,\n",
       "       [0.98946756, 0.01053243],\n",
       "       [0.9121358 , 0.08786421],\n",
       "       [0.91791207, 0.08208794]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097edd79-234e-450a-a28a-e1b5d58c53c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
