{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fa2ec2-c980-412b-95b9-08af56f552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pandas.core.frame import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder, RareLabelEncoder\n",
    "from feature_engine.creation import MathFeatures\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "from feature_engine.selection import (\n",
    "    SmartCorrelatedSelection,\n",
    "    DropHighPSIFeatures,\n",
    "    SelectBySingleFeaturePerformance,\n",
    ")\n",
    "from feature_engine.wrappers import SklearnTransformerWrapper\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = None\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735b9a3c-0521-48e3-93cf-db24420a7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Getting data =======================\n",
    "def reduce_memory_usage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Reduced memory usage by downcasting datatype of columns.\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\"\"\"\n",
    "\n",
    "    # Downcasting dataframe\n",
    "    for column in df:\n",
    "        if df[column].dtype in [\"float64\", \"float32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"float\")\n",
    "        if df[column].dtype in [\"int64\", \"int32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(name: str) -> DataFrame:\n",
    "    \"\"\"Loads DataFrame from csv and reduces used memory.\n",
    "    Parameters: name (the name of csv file without .csv extension)\n",
    "    Returns: DataFrame\"\"\"\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{name}.csv loading\")\n",
    "    df = pd.read_csv(f\"{name}.csv\")\n",
    "    memory = df.memory_usage().sum() / 1024**2\n",
    "    df = reduce_memory_usage(df)\n",
    "    print(\n",
    "        f\"memory usage reduced from {memory:.1f}MB to {(df.memory_usage().sum() / 1024**2):.1f}MB\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Initial pipe =======================\n",
    "\n",
    "def organization_replacer(value: any) -> any:\n",
    "    \"\"\"Reduces the number of unique values\n",
    "    where there are subcategories with ':' sign\"\"\"\n",
    "\n",
    "    if value not in [np.nan, None]:\n",
    "        x = value.split()[0]\n",
    "        if x[-1] == \":\":\n",
    "            return x[:-1]\n",
    "        elif x == \"Business\":\n",
    "            return \"Business\"\n",
    "    return value\n",
    "\n",
    "def organization(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replaces organizations and reduces their numbers\n",
    "    in ORGANIZATION_TYPE column.\"\"\"\n",
    "    \n",
    "    df[\"ORGANIZATION_TYPE\"] = df[\"ORGANIZATION_TYPE\"].map(organization_replacer)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_education(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assigns ordinality to NAME_EDUCATION_TYPE column\"\"\"\n",
    "    \n",
    "    education = {\n",
    "        \"Secondary / secondary special\": 1,\n",
    "        \"Higher education\": 3,\n",
    "        \"Incomplete higher\": 2,\n",
    "        \"Lower secondary\": 0,\n",
    "        \"Academic degree\": 4,\n",
    "    }\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = df[\"NAME_EDUCATION_TYPE\"].map(education)\n",
    "    \n",
    "    return  df\n",
    "\n",
    "\n",
    "def gender_replacer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encodes CODE_GENDER column.\"\"\"\n",
    "    \n",
    "    df[\"CODE_GENDER\"].replace({\"XNA\": np.nan, \"M\": 0, \"F\": 1}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sign_change(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Changes sign of chosen columns.\"\"\"\n",
    "    \n",
    "    for col in [\"DAYS_BIRTH\", \"DAYS_LAST_PHONE_CHANGE\",\n",
    "                \"DAYS_ID_PUBLISH\", \"DAYS_EMPLOYED\"]:\n",
    "        df[col] = df[col].apply(lambda x: x*(-1))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Feature pipe =======================\n",
    "\n",
    "def devision(x: List) -> int:\n",
    "    \"\"\"Devides two features from the list\n",
    "    avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return x[0] / (x[1] + 0.001)\n",
    "\n",
    "\n",
    "def sum_dev(x: List) -> int:\n",
    "    \"\"\"Performs three features math operation\n",
    "    from the list avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return (x[0] + x[1]) * x[2] / 2\n",
    "\n",
    "\n",
    "def weighted_mul(x: List) -> int:\n",
    "    \"\"\"Gets weighted sum of three values in a list.\"\"\"\n",
    "    \n",
    "    return x[0] * 2 + x[1] * 3 + x[2] * 4\n",
    "\n",
    "\n",
    "def remove_special_chars(s: str) -> str:\n",
    "    \"\"\"Replaces special characters from string with '_'.\"\"\"\n",
    "\n",
    "    return \"\".join(e if e.isalnum() else \"_\" for e in s)\n",
    "\n",
    "\n",
    "def standardize_col_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes special characters from the \"\"\"\n",
    "    \n",
    "    return df.rename(columns=remove_special_chars)\n",
    "\n",
    "\n",
    "def nn_mean(x: DataFrame, X_train_prep: DataFrame, y_train: pd.Series) -> DataFrame:\n",
    "    \"\"\"Adds two columns to DataFrame of mean values of target for 50 and 100\n",
    "    nearest neighbors od the poin from training set.\n",
    "    Parameters: x (DataFrame to be transformer),\n",
    "                X_train_prep (preprocessed DataFrame to be fitted to NearestNeighbors model)\n",
    "                y_train (Series of target values to be used to calculate means).\"\"\"\n",
    "    \n",
    "    # Getting columns of interest\n",
    "    columns_of_int = [\n",
    "        \"EXT_SOURCE_1\",\n",
    "        \"EXT_SOURCE_2\",\n",
    "        \"EXT_SOURCE_3\",\n",
    "        \"AMT_CREDIT\",\n",
    "        \"AMT_ANNUITY\",\n",
    "    ]\n",
    "    # Getting data for fitting\n",
    "    df_nn = X_train_prep[columns_of_int]\n",
    "    df_nn[\"CREDIT_ANNUITY_RATIO\"] =  df_nn[\"AMT_CREDIT\"] / (df_nn[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # Getting data for neighbors\n",
    "    df_get = x[columns_of_int]\n",
    "    df_get[\"CREDIT_ANNUITY_RATIO\"] =  df_get[\"AMT_CREDIT\"] / (df_get[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # 50 neighbors\n",
    "    # Fitting model with 50 neighbors\n",
    "    nn_50 = NearestNeighbors(n_neighbors=50).fit(df_nn)\n",
    "    # Indices of neighbours\n",
    "    train_50_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    # Calculating means\n",
    "    new_column_1 = [y_train.iloc[ind].mean() for ind in train_50_neighbours]\n",
    "    # Adding column\n",
    "    x[\"MEAN_50_NN\"] = new_column_1\n",
    "\n",
    "    # 100 neighbors\n",
    "    nn_100 = NearestNeighbors(n_neighbors=100).fit(df_nn)\n",
    "    train_100_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    new_column_2 = [y_train.iloc[ind].mean() for ind in train_100_neighbours]\n",
    "    x[\"MEAN_100_NN\"] = new_column_2\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# ======================= Merging pipe =======================\n",
    "def merging(df):\n",
    "    df = df.merge(merged, on='SK_ID_CURR', how='left')\n",
    "    df = df.fillna(0)\n",
    "    return df \n",
    "\n",
    "\n",
    "def getting_model_columns(df):\n",
    "\n",
    "    # Read column names from the text file\n",
    "    with open('column_names.txt', 'r') as file:\n",
    "        column_names = file.read().splitlines()\n",
    "    return df[column_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ac8b29-e4f6-41a6-b503-3e1e5e1bd67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "application_train.csv loading\n",
      "memory usage reduced from 286.2MB to 129.3MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "app = load_data(\"application_train\")\n",
    "merged = pd.read_csv('preprocessed_data/merged.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c52153-1731-4882-9c00-ae846712391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in SK_ID_CURR of test and validation set \n",
    "with open('test_sk_id_curr.txt', 'r') as f:\n",
    "    test_val_ids = []\n",
    "    for line in f:\n",
    "        sk_id_curr = int(line.strip())\n",
    "        test_val_ids.append(sk_id_curr)\n",
    "\n",
    "# Getting training data\n",
    "app_train = app.loc[~app['SK_ID_CURR'].isin(test_val_ids)]\n",
    "X_train = app_train.drop(['TARGET'], axis=1)\n",
    "y_train = app_train['TARGET']\n",
    "\n",
    "# Loading pipelines from engineering\n",
    "with open('initial_pipe.pkl', 'rb') as file:\n",
    "    initial_pipe = pickle.load(file)\n",
    "\n",
    "with open('preprocess_pipe.pkl', 'rb') as file:\n",
    "    preprocess_pipe = pickle.load(file)\n",
    "\n",
    "with open('feature_pipe.pkl', 'rb') as file:\n",
    "    feature_pipe = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adf5667-7580-4df1-90c9-97e5fa41c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_initial = initial_pipe.transform(X_train)\n",
    "X_preprocessed = preprocess_pipe.transform(X_initial)\n",
    "X_features = feature_pipe.transform(X_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf9b691-ef38-42fe-8f54-06dc5d1c21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "merging_pipe = Pipeline(steps=[\n",
    "                            # Merging with data from other sources\n",
    "                            ('merging', FunctionTransformer(merging)),\n",
    "                            # Getting columns that are used for model training\n",
    "                            ('getting_model_columns', FunctionTransformer(getting_model_columns))\n",
    "])\n",
    "\n",
    "X_merged = merging_pipe.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33dfa9a3-5481-40ed-bb5b-c555b4bee596",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fed0d8-eb85-4555-bd77-2f11e03e373a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9896145 , 0.01038554],\n",
       "       [0.94888335, 0.05111665],\n",
       "       [0.9904215 , 0.00957855],\n",
       "       ...,\n",
       "       [0.9080734 , 0.09192657],\n",
       "       [0.7530327 , 0.2469673 ],\n",
       "       [0.9107421 , 0.08925792]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c7f28-6076-4ada-bc7c-1fdfda33c037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
