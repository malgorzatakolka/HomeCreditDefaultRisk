{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fa2ec2-c980-412b-95b9-08af56f552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder, RareLabelEncoder\n",
    "from feature_engine.creation import MathFeatures\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735b9a3c-0521-48e3-93cf-db24420a7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= Getting data =======================\n",
    "def reduce_memory_usage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Reduced memory usage by downcasting datatype of columns.\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\"\"\"\n",
    "\n",
    "    # Downcasting dataframe\n",
    "    for column in df:\n",
    "        if df[column].dtype in [\"float64\", \"float32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"float\")\n",
    "        if df[column].dtype in [\"int64\", \"int32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(name: str) -> DataFrame:\n",
    "    \"\"\"Loads DataFrame from csv and reduces used memory.\n",
    "    Parameters: name (the name of csv file without .csv extension)\n",
    "    Returns: DataFrame\"\"\"\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{name}.csv loading\")\n",
    "    df = pd.read_csv(f\"{name}.csv\")\n",
    "    memory = df.memory_usage().sum() / 1024**2\n",
    "    df = reduce_memory_usage(df)\n",
    "    print(\n",
    "        f\"memory usage reduced from {memory:.1f}MB to {(df.memory_usage().sum() / 1024**2):.1f}MB\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Initial pipe =======================\n",
    "\n",
    "def organization_replacer(value: any) -> any:\n",
    "    \"\"\"Reduces the number of unique values\n",
    "    where there are subcategories with ':' sign\"\"\"\n",
    "\n",
    "    if value not in [np.nan, None]:\n",
    "        x = value.split()[0]\n",
    "        if x[-1] == \":\":\n",
    "            return x[:-1]\n",
    "        elif x == \"Business\":\n",
    "            return \"Business\"\n",
    "    return value\n",
    "\n",
    "def organization(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replaces organizations and reduces their numbers\n",
    "    in ORGANIZATION_TYPE column.\"\"\"\n",
    "    \n",
    "    df[\"ORGANIZATION_TYPE\"] = df[\"ORGANIZATION_TYPE\"].map(organization_replacer)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_education(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assigns ordinality to NAME_EDUCATION_TYPE column\"\"\"\n",
    "    \n",
    "    education = {\n",
    "        \"Secondary / secondary special\": 1,\n",
    "        \"Higher education\": 3,\n",
    "        \"Incomplete higher\": 2,\n",
    "        \"Lower secondary\": 0,\n",
    "        \"Academic degree\": 4,\n",
    "    }\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = df[\"NAME_EDUCATION_TYPE\"].map(education)\n",
    "    \n",
    "    return  df\n",
    "\n",
    "\n",
    "def gender_replacer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encodes CODE_GENDER column.\"\"\"\n",
    "    \n",
    "    df[\"CODE_GENDER\"].replace({\"XNA\": np.nan, \"M\": 0, \"F\": 1}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sign_change(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Changes sign of chosen columns.\"\"\"\n",
    "    \n",
    "    for col in [\"DAYS_BIRTH\", \"DAYS_LAST_PHONE_CHANGE\",\n",
    "                \"DAYS_ID_PUBLISH\", \"DAYS_EMPLOYED\"]:\n",
    "        df[col] = df[col].apply(lambda x: x*(-1))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Feature pipe =======================\n",
    "\n",
    "def devision(x: List) -> int:\n",
    "    \"\"\"Devides two features from the list\n",
    "    avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return x[0] / (x[1] + 0.001)\n",
    "\n",
    "\n",
    "def sum_dev(x: List) -> int:\n",
    "    \"\"\"Performs three features math operation\n",
    "    from the list avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return (x[0] + x[1]) * x[2] / 2\n",
    "\n",
    "\n",
    "def weighted_mul(x: List) -> int:\n",
    "    \"\"\"Gets weighted sum of three values in a list.\"\"\"\n",
    "    \n",
    "    return x[0] * 2 + x[1] * 3 + x[2] * 4\n",
    "\n",
    "\n",
    "def remove_special_chars(s: str) -> str:\n",
    "    \"\"\"Replaces special characters from string with '_'.\"\"\"\n",
    "\n",
    "    return \"\".join(e if e.isalnum() else \"_\" for e in s)\n",
    "\n",
    "\n",
    "def standardize_col_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes special characters from the \"\"\"\n",
    "    \n",
    "    return df.rename(columns=remove_special_chars)\n",
    "\n",
    "\n",
    "def nn_mean(x: DataFrame, X_train_prep: DataFrame, y_train: pd.Series) -> DataFrame:\n",
    "    \"\"\"Adds two columns to DataFrame of mean values of target for 50 and 100\n",
    "    nearest neighbors od the poin from training set.\n",
    "    Parameters: x (DataFrame to be transformer),\n",
    "                X_train_prep (preprocessed DataFrame to be fitted to NearestNeighbors model)\n",
    "                y_train (Series of target values to be used to calculate means).\"\"\"\n",
    "    \n",
    "    # Getting columns of interest\n",
    "    columns_of_int = [\n",
    "        \"EXT_SOURCE_1\",\n",
    "        \"EXT_SOURCE_2\",\n",
    "        \"EXT_SOURCE_3\",\n",
    "        \"AMT_CREDIT\",\n",
    "        \"AMT_ANNUITY\",\n",
    "    ]\n",
    "    # Getting data for fitting\n",
    "    df_nn = X_train_prep[columns_of_int]\n",
    "    df_nn[\"CREDIT_ANNUITY_RATIO\"] =  df_nn[\"AMT_CREDIT\"] / (df_nn[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # Getting data for neighbors\n",
    "    df_get = x[columns_of_int]\n",
    "    df_get[\"CREDIT_ANNUITY_RATIO\"] =  df_get[\"AMT_CREDIT\"] / (df_get[\"AMT_ANNUITY\"]+0.0001)\n",
    "    # 50 neighbors\n",
    "    # Fitting model with 50 neighbors\n",
    "    nn_50 = NearestNeighbors(n_neighbors=50).fit(df_nn)\n",
    "    # Indices of neighbours\n",
    "    train_50_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    # Calculating means\n",
    "    new_column_1 = [y_train.iloc[ind].mean() for ind in train_50_neighbours]\n",
    "    # Adding column\n",
    "    x[\"MEAN_50_NN\"] = new_column_1\n",
    "\n",
    "    # 100 neighbors\n",
    "    nn_100 = NearestNeighbors(n_neighbors=100).fit(df_nn)\n",
    "    train_100_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    new_column_2 = [y_train.iloc[ind].mean() for ind in train_100_neighbours]\n",
    "    x[\"MEAN_100_NN\"] = new_column_2\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# ======================= Merging pipe =======================\n",
    "def merging(df):\n",
    "    \"\"\"Merges data with columns generated through aggregations from \n",
    "    previous applications.\"\"\"\n",
    "    \n",
    "    df = df.merge(merged, on='SK_ID_CURR', how='left')\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "def getting_model_columns(df):\n",
    "    \"\"\"Selects most important columns.\"\"\"\n",
    "    \n",
    "    # Read column names from the text file\n",
    "    with open('column_names.txt', 'r') as file:\n",
    "        column_names = file.read().splitlines()\n",
    "        \n",
    "    return df[column_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26147188-9e80-4e9f-a35a-b75d77e75145",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ac8b29-e4f6-41a6-b503-3e1e5e1bd67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "application_train.csv loading\n",
      "memory usage reduced from 286.2MB to 129.3MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "app = load_data(\"application_train\")\n",
    "merged = pd.read_csv('preprocessed_data/merged.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfdf3d-833c-4757-b34d-6953ffa8ece0",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8683da0e-e777-487a-970d-d8f1a335b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading XGBClassifier that was trained in Modeling part\n",
    "model = joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06927f-5420-443c-9a07-0cfda43a8c25",
   "metadata": {},
   "source": [
    "## Preparation of final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c52153-1731-4882-9c00-ae846712391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in SK_ID_CURR of test and validation set \n",
    "with open('test_sk_id_curr.txt', 'r') as f:\n",
    "    test_val_ids = []\n",
    "    for line in f:\n",
    "        sk_id_curr = int(line.strip())\n",
    "        test_val_ids.append(sk_id_curr)\n",
    "\n",
    "# Getting training data\n",
    "app_train = app.loc[~app['SK_ID_CURR'].isin(test_val_ids)]\n",
    "X_train = app_train.drop(['TARGET'], axis=1)\n",
    "y_train = app_train['TARGET']\n",
    "\n",
    "# Loading pipelines from engineering\n",
    "with open('initial_pipe.pkl', 'rb') as file:\n",
    "    initial_pipe = pickle.load(file)\n",
    "\n",
    "with open('preprocess_pipe.pkl', 'rb') as file:\n",
    "    preprocess_pipe = pickle.load(file)\n",
    "\n",
    "with open('feature_pipe.pkl', 'rb') as file:\n",
    "    feature_pipe = pickle.load(file)\n",
    "\n",
    "# Transforming features\n",
    "X_initial = initial_pipe.transform(X_train)\n",
    "X_preprocessed = preprocess_pipe.transform(X_initial)\n",
    "X_features = feature_pipe.transform(X_preprocessed)\n",
    "\n",
    "# Creating and fitting pipeline for merging and selecting features\n",
    "merging_pipe = Pipeline(steps=[\n",
    "                            # Merging with data from other sources\n",
    "                            ('merging', FunctionTransformer(merging)),\n",
    "                            # Getting columns that are used for model training\n",
    "                            ('getting_model_columns', FunctionTransformer(getting_model_columns))\n",
    "])\n",
    "\n",
    "# Fitting with preprocessed features\n",
    "merging_pipe.fit(X_features)\n",
    "\n",
    "# Storing pipeline\n",
    "pickle.dump(merging_pipe, open('merging_pipe.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4de0a4-2f45-468a-b4c2-f91070fc084f",
   "metadata": {},
   "source": [
    "## Generating and storing final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bb9623-3b62-4445-be68-10f19b133935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a pipeline\n",
    "final_pipeline = Pipeline([\n",
    "    ('initial', initial_pipe),\n",
    "    ('preprocess', preprocess_pipe),\n",
    "    ('feature', feature_pipe),\n",
    "    ('merge', merging_pipe)\n",
    "])\n",
    "\n",
    "# Storing pipeline\n",
    "pickle.dump(final_pipeline, open('final_pipeline.pkl', 'wb'))\n",
    "\n",
    "# Load final pipeline\n",
    "with open('final_pipeline.pkl', 'rb') as file:\n",
    "    final_pipeline = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e775d0-b5e3-4c2d-838b-130491108d15",
   "metadata": {},
   "source": [
    "## Testing final pipeline\n",
    "\n",
    "To test the pipeline we will use 4 random indices from application train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06660f5-04e6-4e7c-9327-3ab82173952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of defaulting: \n",
      "index 294471: 0.2581489086151123\n",
      "index 146033: 0.04179701954126358\n",
      "index 258076: 0.03725864365696907\n",
      "index 170047: 0.08745807409286499\n"
     ]
    }
   ],
   "source": [
    "# Generate four random numbers\n",
    "random_numbers = [random.randint(0, app.shape[0]-1) for _ in range(4)]\n",
    "\n",
    "# Get X out of application_train.csv\n",
    "X = app.drop(['TARGET'], axis=1)\n",
    "\n",
    "# Generate DataFrame for predictions\n",
    "X_testing  = X.iloc[random_numbers, :]\n",
    "\n",
    "# Transforming selected rows with final pipeline\n",
    "X_pipeline = final_pipeline.transform(X_testing)\n",
    "\n",
    "print(\"Probability of defaulting: \")\n",
    "# Predicting probability of defaulting \n",
    "for i, proba in zip(random_numbers, model.predict_proba(X_pipeline)[:, 1]):\n",
    "    print(f\"index {i}: {proba}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dc364-bd83-42cc-b67a-e7af4b45fc82",
   "metadata": {},
   "source": [
    "## Making batch predictions on new entries in application_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8745b9-95b6-465f-a7e5-015e4484ed17",
   "metadata": {},
   "source": [
    "1. Select rows of interest from the dataframe and store in variable ```X_pred```\n",
    "2. Transform with final pipeline: ```X_transformed = final_pipeline.transform(X_pred)```\n",
    "3. Make predictions of defaulting: ```model.predict_proba(X_transformed)```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
