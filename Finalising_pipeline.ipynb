{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b45c16-db35-4243-9249-75fa9114a67c",
   "metadata": {},
   "source": [
    "# Finalising pipeline to transform data for predictions <a class=\"jp-toc-ignore\"></a>\n",
    "\n",
    "The aim of this notebook is to finalize the pipeline that could be used for predicting probability of defaulting for new entries in application_train.csv\n",
    "\n",
    "It also provides a code to generate example dictionary that could be used to make prediction in deployed app using Swagger [https://default-risk-fastapi-f4fhso7e5q-nw.a.run.app/docs](https://default-risk-fastapi-f4fhso7e5q-nw.a.run.app/docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fa2ec2-c980-412b-95b9-08af56f552d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "from pandas.core.frame import DataFrame\n",
    "import json\n",
    "\n",
    "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
    "from feature_engine.encoding import OneHotEncoder, RareLabelEncoder\n",
    "from feature_engine.creation import MathFeatures\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735b9a3c-0521-48e3-93cf-db24420a7cfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ======================= Getting data =======================\n",
    "def reduce_memory_usage(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Reduced memory usage by downcasting datatype of columns.\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\"\"\"\n",
    "\n",
    "    # Downcasting dataframe\n",
    "    for column in df:\n",
    "        if df[column].dtype in [\"float64\", \"float32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"float\")\n",
    "        if df[column].dtype in [\"int64\", \"int32\"]:\n",
    "            df[column] = pd.to_numeric(df[column], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(name: str) -> DataFrame:\n",
    "    \"\"\"Loads DataFrame from csv and reduces used memory.\n",
    "    Parameters: name (the name of csv file without .csv extension)\n",
    "    Returns: DataFrame\"\"\"\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{name}.csv loading\")\n",
    "    df = pd.read_csv(f\"{name}.csv\")\n",
    "    memory = df.memory_usage().sum() / 1024**2\n",
    "    df = reduce_memory_usage(df)\n",
    "    print(\n",
    "        f\"memory usage reduced from {memory:.1f}MB to {(df.memory_usage().sum() / 1024**2):.1f}MB\"\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Initial pipe =======================\n",
    "\n",
    "\n",
    "def organization_replacer(value: any) -> any:\n",
    "    \"\"\"Reduces the number of unique values\n",
    "    where there are subcategories with ':' sign\"\"\"\n",
    "\n",
    "    if value not in [np.nan, None]:\n",
    "        x = value.split()[0]\n",
    "        if x[-1] == \":\":\n",
    "            return x[:-1]\n",
    "        elif x == \"Business\":\n",
    "            return \"Business\"\n",
    "    return value\n",
    "\n",
    "\n",
    "def organization(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Replaces organizations and reduces their numbers\n",
    "    in ORGANIZATION_TYPE column.\"\"\"\n",
    "\n",
    "    df[\"ORGANIZATION_TYPE\"] = df[\"ORGANIZATION_TYPE\"].map(organization_replacer)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_education(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Assigns ordinality to NAME_EDUCATION_TYPE column\"\"\"\n",
    "\n",
    "    education = {\n",
    "        \"Secondary / secondary special\": 1,\n",
    "        \"Higher education\": 3,\n",
    "        \"Incomplete higher\": 2,\n",
    "        \"Lower secondary\": 0,\n",
    "        \"Academic degree\": 4,\n",
    "    }\n",
    "    df[\"NAME_EDUCATION_TYPE\"] = df[\"NAME_EDUCATION_TYPE\"].replace(education)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def gender_replacer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Encodes CODE_GENDER column.\"\"\"\n",
    "\n",
    "    df[\"CODE_GENDER\"].replace({\"XNA\": np.nan, \"M\": 0, \"F\": 1}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sign_change(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Changes sign of chosen columns.\"\"\"\n",
    "\n",
    "    for col in [\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_LAST_PHONE_CHANGE\",\n",
    "        \"DAYS_ID_PUBLISH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ]:\n",
    "        df[col] = df[col].apply(lambda x: x * (-1))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================= Feature pipe =======================\n",
    "\n",
    "\n",
    "def devision(x: List) -> int:\n",
    "    \"\"\"Devides two features from the list\n",
    "    avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return x[0] / (x[1] + 0.001)\n",
    "\n",
    "\n",
    "def sum_dev(x: List) -> int:\n",
    "    \"\"\"Performs three features math operation\n",
    "    from the list avoiding ZeroDevisionError.\"\"\"\n",
    "\n",
    "    return (x[0] + x[1]) * x[2] / 2\n",
    "\n",
    "\n",
    "def weighted_mul(x: List) -> int:\n",
    "    \"\"\"Gets weighted sum of three values in a list.\"\"\"\n",
    "\n",
    "    return x[0] * 2 + x[1] * 3 + x[2] * 4\n",
    "\n",
    "\n",
    "def remove_special_chars(s: str) -> str:\n",
    "    \"\"\"Replaces special characters from string with '_'.\"\"\"\n",
    "\n",
    "    return \"\".join(e if e.isalnum() else \"_\" for e in s)\n",
    "\n",
    "\n",
    "def standardize_col_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes special characters from the\"\"\"\n",
    "\n",
    "    return df.rename(columns=remove_special_chars)\n",
    "\n",
    "\n",
    "def nn_mean(x: DataFrame, X_train_prep: DataFrame, y_train: pd.Series) -> DataFrame:\n",
    "    \"\"\"Adds two columns to DataFrame of mean values of target for 50 and 100\n",
    "    nearest neighbors od the poin from training set.\n",
    "    Parameters: x (DataFrame to be transformer),\n",
    "                X_train_prep (preprocessed DataFrame to be fitted to NearestNeighbors model)\n",
    "                y_train (Series of target values to be used to calculate means).\"\"\"\n",
    "\n",
    "    # Getting columns of interest\n",
    "    columns_of_int = [\n",
    "        \"EXT_SOURCE_1\",\n",
    "        \"EXT_SOURCE_2\",\n",
    "        \"EXT_SOURCE_3\",\n",
    "        \"AMT_CREDIT\",\n",
    "        \"AMT_ANNUITY\",\n",
    "    ]\n",
    "    # Getting data for fitting\n",
    "    df_nn = X_train_prep[columns_of_int]\n",
    "    df_nn[\"CREDIT_ANNUITY_RATIO\"] = df_nn[\"AMT_CREDIT\"] / (\n",
    "        df_nn[\"AMT_ANNUITY\"] + 0.0001\n",
    "    )\n",
    "    # Getting data for neighbors\n",
    "    df_get = x[columns_of_int]\n",
    "    df_get[\"CREDIT_ANNUITY_RATIO\"] = df_get[\"AMT_CREDIT\"] / (\n",
    "        df_get[\"AMT_ANNUITY\"] + 0.0001\n",
    "    )\n",
    "    # 50 neighbors\n",
    "    # Fitting model with 50 neighbors\n",
    "    nn_50 = NearestNeighbors(n_neighbors=50).fit(df_nn)\n",
    "    # Indices of neighbours\n",
    "    train_50_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    # Calculating means\n",
    "    new_column_1 = [y_train.iloc[ind].mean() for ind in train_50_neighbours]\n",
    "    # Adding column\n",
    "    x[\"MEAN_50_NN\"] = new_column_1\n",
    "\n",
    "    # 100 neighbors\n",
    "    nn_100 = NearestNeighbors(n_neighbors=100).fit(df_nn)\n",
    "    train_100_neighbours = nn_50.kneighbors(df_get)[1]\n",
    "    new_column_2 = [y_train.iloc[ind].mean() for ind in train_100_neighbours]\n",
    "    x[\"MEAN_100_NN\"] = new_column_2\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# ======================= Merging pipe =======================\n",
    "def merging(df):\n",
    "    \"\"\"Merges data with columns generated through aggregations from\n",
    "    previous applications.\"\"\"\n",
    "\n",
    "    df = df.merge(merged, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def getting_model_columns(df):\n",
    "    \"\"\"Selects most important columns.\"\"\"\n",
    "\n",
    "    # Read column names from the text file\n",
    "    with open(\"column_names.txt\", \"r\") as file:\n",
    "        column_names = file.read().splitlines()\n",
    "\n",
    "    return df[column_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26147188-9e80-4e9f-a35a-b75d77e75145",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ac8b29-e4f6-41a6-b503-3e1e5e1bd67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "application_train.csv loading\n",
      "memory usage reduced from 286.2MB to 129.3MB\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "app = load_data(\"application_train\")\n",
    "merged = pd.read_csv(\"preprocessed_data/merged.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfdf3d-833c-4757-b34d-6953ffa8ece0",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8683da0e-e777-487a-970d-d8f1a335b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading XGBClassifier that was trained in Modeling part\n",
    "model = joblib.load(\"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06927f-5420-443c-9a07-0cfda43a8c25",
   "metadata": {},
   "source": [
    "# Preparation of final pipeline\n",
    "\n",
    "As this step is done only once and the final pipeline is stored, the code is commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c52153-1731-4882-9c00-ae846712391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in SK_ID_CURR of test and validation set\n",
    "# with open(\"test_sk_id_curr.txt\", \"r\") as f:\n",
    "#     test_val_ids = []\n",
    "#     for line in f:\n",
    "#         sk_id_curr = int(line.strip())\n",
    "#         test_val_ids.append(sk_id_curr)\n",
    "\n",
    "# # Getting training data\n",
    "# app_train = app.loc[~app[\"SK_ID_CURR\"].isin(test_val_ids)]\n",
    "# X_train = app_train.drop([\"TARGET\"], axis=1)\n",
    "# y_train = app_train[\"TARGET\"]\n",
    "\n",
    "# # Loading pipelines from engineering\n",
    "# with open(\"initial_pipe.pkl\", \"rb\") as file:\n",
    "#     initial_pipe = pickle.load(file)\n",
    "\n",
    "# with open(\"preprocess_pipe.pkl\", \"rb\") as file:\n",
    "#     preprocess_pipe = pickle.load(file)\n",
    "\n",
    "# with open(\"feature_pipe.pkl\", \"rb\") as file:\n",
    "#     feature_pipe = pickle.load(file)\n",
    "\n",
    "# # Transforming features\n",
    "# X_initial = initial_pipe.transform(X_train)\n",
    "# X_preprocessed = preprocess_pipe.transform(X_initial)\n",
    "# X_features = feature_pipe.transform(X_preprocessed)\n",
    "\n",
    "# # Creating and fitting pipeline for merging and selecting features\n",
    "# merging_pipe = Pipeline(\n",
    "#     steps=[\n",
    "#         # Merging with data from other sources\n",
    "#         (\"merging\", FunctionTransformer(merging)),\n",
    "#         # Getting columns that are used for model training\n",
    "#         (\"getting_model_columns\", FunctionTransformer(getting_model_columns)),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Fitting with preprocessed features\n",
    "# merging_pipe.fit(X_features)\n",
    "\n",
    "# # Storing pipeline\n",
    "# pickle.dump(merging_pipe, open(\"merging_pipe.pkl\", \"wb\"))\n",
    "\n",
    "# # Making a final pipeline\n",
    "# final_pipeline = Pipeline(\n",
    "#     [\n",
    "#         (\"initial\", initial_pipe),\n",
    "#         (\"preprocess\", preprocess_pipe),\n",
    "#         (\"feature\", feature_pipe),\n",
    "#         (\"merge\", merging_pipe),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Storing pipeline\n",
    "# pickle.dump(final_pipeline, open(\"final_pipeline.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4de0a4-2f45-468a-b4c2-f91070fc084f",
   "metadata": {},
   "source": [
    "# Loading final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bb9623-3b62-4445-be68-10f19b133935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final pipeline\n",
    "with open(\"final_pipeline.pkl\", \"rb\") as file:\n",
    "    final_pipeline = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e775d0-b5e3-4c2d-838b-130491108d15",
   "metadata": {},
   "source": [
    "# Testing final pipeline\n",
    "\n",
    "To test the pipeline we will use 4 random indices from application train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06660f5-04e6-4e7c-9327-3ab82173952d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of defaulting: \n",
      "index 142105: 0.011837313883006573\n",
      "index 8373: 0.006339050829410553\n",
      "index 27306: 0.021818062290549278\n",
      "index 205324: 0.023363055661320686\n"
     ]
    }
   ],
   "source": [
    "# Generate four random numbers\n",
    "random_numbers = [random.randint(0, app.shape[0] - 1) for _ in range(4)]\n",
    "\n",
    "# Get X out of application_train.csv\n",
    "X = app.drop([\"TARGET\"], axis=1)\n",
    "\n",
    "# Generate DataFrame for predictions\n",
    "X_testing = X.iloc[random_numbers, :]\n",
    "\n",
    "# Transforming selected rows with final pipeline\n",
    "X_pipeline = final_pipeline.transform(X_testing)\n",
    "\n",
    "print(\"Probability of defaulting: \")\n",
    "# Predicting probability of defaulting\n",
    "for i, proba in zip(random_numbers, model.predict_proba(X_pipeline)[:, 1]):\n",
    "    print(f\"index {i}: {proba}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4dc364-bd83-42cc-b67a-e7af4b45fc82",
   "metadata": {},
   "source": [
    "# Making batch predictions on new entries in application_train locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8745b9-95b6-465f-a7e5-015e4484ed17",
   "metadata": {},
   "source": [
    "1. Select rows of interest from the dataframe and store in variable ```X_pred```\n",
    "2. Transform with final pipeline: ```X_transformed = final_pipeline.transform(X_pred)```\n",
    "3. Make predictions of defaulting: ```model.predict_proba(X_transformed)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d09d3-cbef-4d1e-9654-f9a00af44429",
   "metadata": {},
   "source": [
    "# Generating input for API predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d2744-8c49-438d-ad82-04ae8b0ae2d0",
   "metadata": {},
   "source": [
    "This section provides an example code to generate entries for predictions with deployed app at [https://default-risk-fastapi-f4fhso7e5q-nw.a.run.app/docs](https://default-risk-fastapi-f4fhso7e5q-nw.a.run.app/docs)\n",
    "\n",
    "* Go to the API\n",
    "* Click *predict*\n",
    "* Click *Try it out*\n",
    "* Generate the input with code\n",
    "\n",
    "```python\n",
    "# Choosing a random entry\n",
    "X_api = X.iloc[[123], :]\n",
    "\n",
    "# Transforming\n",
    "X_api = final_pipeline.transform(X_api)\n",
    "\n",
    "# Store in dictionary\n",
    "X_api = X_api.to_dict(orient='record')\n",
    "```\n",
    "* Copy the dictionary and past to try it out\n",
    "* Get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65feda17-b88f-41de-be3c-ac114339ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CODE_GENDER\": 1.0, \"AMT_INCOME_TOTAL\": 112500.0, \"AMT_CREDIT\": 535500.0, \"AMT_ANNUITY\": 30028.5, \"AMT_GOODS_PRICE\": 535500.0, \"NAME_EDUCATION_TYPE\": 1.0, \"REGION_POPULATION_RELATIVE\": 0.028663000091910362, \"DAYS_BIRTH\": 16759.0, \"DAYS_EMPLOYED\": 4560.0, \"OWN_CAR_AGE\": 9.0, \"FLAG_EMP_PHONE\": 1.0, \"REGION_RATING_CLIENT\": 2.0, \"HOUR_APPR_PROCESS_START\": 4.0, \"REG_CITY_NOT_LIVE_CITY\": 0.0, \"REG_CITY_NOT_WORK_CITY\": 0.0, \"LIVE_CITY_NOT_WORK_CITY\": 0.0, \"EXT_SOURCE_1\": 0.5057128667831421, \"EXT_SOURCE_2\": 0.6116366982460022, \"APARTMENTS_AVG\": 0.08449999988079071, \"YEARS_BEGINEXPLUATATION_AVG\": 0.9757000207901001, \"YEARS_BUILD_AVG\": 0.6668000221252441, \"ELEVATORS_AVG\": 0.0, \"FLOORSMAX_AVG\": 0.16670000553131104, \"FLOORSMIN_AVG\": 0.20829999446868896, \"LANDAREA_AVG\": 0.042100001126527786, \"LIVINGAREA_AVG\": 0.0640999972820282, \"APARTMENTS_MODE\": 0.08609999716281891, \"BASEMENTAREA_MODE\": 0.06750000268220901, \"YEARS_BEGINEXPLUATATION_MODE\": 0.9757000207901001, \"YEARS_BUILD_MODE\": 0.6797999739646912, \"ELEVATORS_MODE\": 0.0, \"ENTRANCES_MODE\": 0.1378999948501587, \"FLOORSMAX_MODE\": 0.16670000553131104, \"LANDAREA_MODE\": 0.0430000014603138, \"LIVINGAPARTMENTS_MODE\": 0.06610000133514404, \"LIVINGAREA_MODE\": 0.06679999828338623, \"APARTMENTS_MEDI\": 0.08540000021457672, \"BASEMENTAREA_MEDI\": 0.06509999930858612, \"YEARS_BUILD_MEDI\": 0.6712999939918518, \"FLOORSMAX_MEDI\": 0.16670000553131104, \"LANDAREA_MEDI\": 0.04280000180006027, \"LIVINGAREA_MEDI\": 0.06520000100135803, \"TOTALAREA_MODE\": 0.08330000191926956, \"DEF_30_CNT_SOCIAL_CIRCLE\": 2.0, \"DAYS_LAST_PHONE_CHANGE\": 1282.0, \"FLAG_DOCUMENT_3\": 1.0, \"AMT_REQ_CREDIT_BUREAU_YEAR\": 1.0, \"NAME_INCOME_TYPE_Pensioner\": 0.0, \"NAME_FAMILY_STATUS_Married\": 0.0, \"OCCUPATION_TYPE_Laborers\": 0.0, \"FONDKAPREMONT_MODE_XNA\": 0.0, \"HOUSETYPE_MODE_block_of_flats\": 1.0, \"HOUSETYPE_MODE_XNA\": 0.0, \"EMERGENCYSTATE_MODE_No\": 1.0, \"EMERGENCYSTATE_MODE_XNA\": 0.0, \"RATIO_ANNUITY_TO_INCOME\": 0.2669199976273778, \"REGION_TO_INCOME\": 225000.0, \"RATIO_CREDIT_TO_EXT_SOURCE\": 995223.7135635528, \"SUM_EXT_SOURCES\": 1.6544196605682373, \"MAX_EXTERNAL_SOURCES\": 0.6116366982460022, \"MIN_EXT_SOURCES\": 0.5057128667831421, \"PROD_EXT_SOURCES\": 0.16612248122692108, \"CREDIT_ANNUITY_RATIO\": 17.83305800046429, \"PROD_REGION_POPULATION_AMT_CREDIT\": 15349.0361328125, \"PROD_REGION_RATING_AMT_INCOME\": 225000.0, \"INCOME_PER_CHILD\": 112500000.0, \"INCOME_PER_PERSON\": 112387.6123876124, \"PAYMENT_RATE\": 0.05607563014738444, \"MEAN_50_NN\": 0.04, \"FRESH_MONTHS_BALANCE_MEAN_MIN_ACTIVE\": 0.0, \"FRESH_MONTHS_BALANCE_MEAN_MAX_ACTIVE\": 0.0, \"CREDIT_COUNT_ACTIVE\": 0.0, \"CREDIT_COUNT_CLOSED\": 0.0, \"FRESH_MONTHS_BALANCE_MIN\": 1.0, \"FRESH_SK_DPD_DEF_SUM\": 0.0, \"OLD_MONTHS_BALANCE_MAX\": 42.0, \"OLD_CNT_INSTALMENT_MEAN\": 12.0, \"OLD_CNT_INSTALMENT_SUM\": 84.0, \"OLD_CNT_INSTALMENT_FUTURE_MEAN\": 9.0, \"OLD_SK_DPD_COUNT\": 7.0, \"OLD_SK_DPD_DEF_COUNT\": 7.0, \"NAME_CONTRACT_STATUS_ACTIVE\": 43.0, \"NAME_CONTRACT_STATUS_COMPLETED\": 3.0, \"FRESH_NUM_INSTALMENT_VERSION_MIN\": 1.0, \"FRESH_NUM_INSTALMENT_NUMBER_MAX\": 18.0, \"FRESH_AMT_INSTALMENT_MIN\": 4.14, \"FRESH_AMT_PAYMENT_MIN\": 8058.195, \"FRESH_DIFF_DAYS_PAYMNET_MIN\": -1.0, \"OLD_NUM_INSTALMENT_VERSION_SUM\": 6.0, \"OLD_NUM_INSTALMENT_VERSION_MIN\": 1.0, \"OLD_NUM_INSTALMENT_NUMBER_MEAN\": 3.5, \"OLD_NUM_INSTALMENT_NUMBER_MAX\": 6.0, \"OLD_NUM_INSTALMENT_NUMBER_SUM\": 21.0, \"OLD_NUM_INSTALMENT_NUMBER_MIN\": 1.0, \"OLD_DAYS_INSTALMENT_MEAN\": -1177.0, \"OLD_DAYS_INSTALMENT_MAX\": -1102.0, \"OLD_DAYS_INSTALMENT_SUM\": -7062.0, \"OLD_DAYS_INSTALMENT_MIN\": -1252.0, \"OLD_DAYS_ENTRY_PAYMENT_MAX\": -1103.0, \"OLD_DAYS_ENTRY_PAYMENT_SUM\": -7073.0, \"OLD_DAYS_ENTRY_PAYMENT_MIN\": -1256.0, \"OLD_AMT_INSTALMENT_MEAN\": 8086.32, \"OLD_AMT_INSTALMENT_MAX\": 8086.32, \"OLD_AMT_INSTALMENT_SUM\": 48517.92, \"OLD_AMT_INSTALMENT_MIN\": 8086.32, \"OLD_AMT_PAYMENT_MEAN\": 8086.32, \"OLD_AMT_PAYMENT_MAX\": 8086.32, \"OLD_AMT_PAYMENT_SUM\": 48517.92, \"OLD_AMT_PAYMENT_MIN\": 8086.32, \"OLD_DIFF_DAYS_PAYMNET_MEAN\": 1.8333334, \"OLD_DIFF_DAYS_PAYMNET_SUM\": 11.0, \"OLD_DIFF_DAYS_PAYMNET_MIN\": 0.0, \"OLD_RATIO_AMT_PAYMENT_MEAN\": 1.0, \"OLD_RATIO_AMT_PAYMENT_MAX\": 1.0, \"OLD_RATIO_AMT_PAYMENT_SUM\": 6.0, \"CNT_PAYMENT_MIN\": 0.0, \"CNT_PAYMENT_MAX\": 0.0, \"CNT_PAYMENT_MEAN\": 0.0, \"HOUR_APPR_PROCESS_START_MIN\": 7.0, \"DAYS_DECISION_MAX\": -100.0, \"SELLERPLACE_AREA_MAX\": -1.0, \"SK_ID_PREV_COUNT\": 6.0, \"PRODUCT_COMBINATION_RARE_SUM\": 0.0, \"PRODUCT_COMBINATION_CARD_STREET_SUM\": 0.0, \"PRODUCT_COMBINATION_CARD_X_SELL_SUM\": 0.0, \"NAME_YIELD_GROUP_XNA_SUM\": 6.0, \"NAME_YIELD_GROUP_MIDDLE_SUM\": 0.0, \"NAME_CONTRACT_STATUS_REFUSED_SUM\": 0.0, \"CODE_REJECT_REASON_XAP_SUM\": 6.0, \"CODE_REJECT_REASON_LIMIT_SUM\": 0.0, \"CODE_REJECT_REASON_HC_SUM\": 0.0, \"CODE_REJECT_REASON_SCOFR_SUM\": 0.0, \"CHANNEL_TYPE_CREDIT_AND_CASH_OFFICES_SUM\": 5.0, \"CHANNEL_TYPE_AP___CASH_LOAN__SUM\": 0.0, \"NAME_PRODUCT_TYPE_XNA_SUM\": 6.0, \"NAME_PRODUCT_TYPE_WALK_IN_SUM\": 0.0, \"NAME_GOODS_CATEGORY_XNA_SUM\": 6.0, \"NAME_CASH_LOAN_PURPOSE_XNA_SUM\": 6.0, \"NAME_CASH_LOAN_PURPOSE_XAP_SUM\": 0.0, \"NAME_CASH_LOAN_PURPOSE_RARE_SUM\": 0.0, \"NAME_PORTFOLIO_XNA_SUM\": 6.0, \"NAME_PORTFOLIO_CASH_SUM\": 0.0, \"NAME_PORTFOLIO_CARDS_SUM\": 0.0, \"NAME_CONTRACT_TYPE_REVOLVING_LOANS_SUM\": 0.0, \"NAME_PAYMENT_TYPE_CASH_THROUGH_THE_BANK_SUM\": 0.0, \"NAME_CLIENT_TYPE_REPEATER_SUM\": 6.0, \"NAME_TYPE_SUITE_XNA_SUM\": 6.0, \"NAME_TYPE_SUITE_UNACCOMPANIED_SUM\": 0.0, \"WEEKDAY_APPR_PROCESS_START_FRIDAY_SUM\": 3.0, \"WEEKDAY_APPR_PROCESS_START_THURSDAY_SUM\": 0.0, \"FRESH_MONTHS_BALANCE_MEAN\": 18.0, \"FRESH_AMT_CREDIT_LIMIT_ACTUAL_MIN\": 45000.0, \"FRESH_AMT_CREDIT_LIMIT_ACTUAL_SUM\": 5827500.0, \"FRESH_AMT_DRAWINGS_ATM_CURRENT_MAX\": 0.0, \"FRESH_AMT_DRAWINGS_ATM_CURRENT_SUM\": 0.0, \"FRESH_AMT_PAYMENT_CURRENT_MIN\": 0.0, \"FRESH_CNT_DRAWINGS_ATM_CURRENT_MEAN\": 0.0, \"FRESH_CNT_DRAWINGS_ATM_CURRENT_MAX\": 0.0, \"FRESH_CNT_DRAWINGS_ATM_CURRENT_SUM\": 0.0, \"FRESH_CNT_DRAWINGS_CURRENT_MAX\": 0.0, \"FRESH_CNT_INSTALMENT_MATURE_CUM_MAX\": 0.0, \"FRESH_CNT_INSTALMENT_MATURE_CUM_SUM\": 0.0, \"OLD_MONTHS_BALANCE_SUM\": 273.0, \"OLD_AMT_BALANCE_MEAN\": 0.0, \"OLD_AMT_CREDIT_LIMIT_ACTUAL_MAX\": 247500.0, \"OLD_AMT_CREDIT_LIMIT_ACTUAL_SUM\": 1732500.0, \"OLD_AMT_DRAWINGS_ATM_CURRENT_MEAN\": 0.0, \"OLD_AMT_INST_MIN_REGULARITY_MAX\": 0.0, \"OLD_AMT_PAYMENT_CURRENT_SUM\": 0.0, \"OLD_AMT_RECEIVABLE_PRINCIPAL_SUM\": 0.0, \"OLD_CNT_DRAWINGS_ATM_CURRENT_MEAN\": 0.0, \"OLD_CNT_DRAWINGS_ATM_CURRENT_SUM\": 0.0, \"OLD_CNT_DRAWINGS_CURRENT_MEAN\": 0.0, \"OLD_CNT_DRAWINGS_CURRENT_MAX\": 0.0, \"OLD_SK_DPD_MAX\": 0.0, \"OLD_SK_DPD_DEF_SUM\": 0.0, \"OLD_BALANCE_LIMIT_RATIO_MAX\": 0.0, \"OLD_AMT_DRAWING_ALL_MEAN\": 0.0, \"OLD_AMT_DRAWING_ALL_MAX\": 0.0}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choosing a random entry\n",
    "X_api = X.iloc[[123], :]\n",
    "\n",
    "# Transforming\n",
    "X_api = final_pipeline.transform(X_api).astype(float)\n",
    "\n",
    "# Store in dictionary\n",
    "X_api = X_api.to_dict(orient=\"record\")[0]\n",
    "json_X_api = json.dumps(X_api)\n",
    "json_X_api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ead93-99a2-4291-92e8-9d6c44860771",
   "metadata": {},
   "source": [
    "Getting the output with predictive probability of defaulting: \n",
    "\n",
    "![default](images/default_risk_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a634290-ea0b-4340-863e-80ff6d2a6df5",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accfa56-38c9-4a30-a637-864442dbbb44",
   "metadata": {},
   "source": [
    "The business needs are not define in the requirements of the project. Creating pipeline with preprocessing, feature engineering and predictive model is possible but would require more computational power and would generate additional costs on GCP that's why the transformations are done locally with an option to call the model with prepared data. The solution may not be ideall but good enough for exploring possibilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
